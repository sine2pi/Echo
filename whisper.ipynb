{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import functools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import aiohttp\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, amp, optim, nn\n",
    "import evaluate\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from threading import Thread\n",
    "from typing import Dict, Optional, Tuple, Union, List, Any\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, PretrainedConfig, TrainerCallback,\n",
    "    WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizerFast\n",
    ")\n",
    "\n",
    "from evaluate import module\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, IterableDatasetDict, Audio, DatasetDict\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional, Tuple\n",
    "\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from whisper.decoding import decode as decode_function\n",
    "from whisper.decoding import detect_language as detect_language_function\n",
    "from whisper.transcribe import transcribe as transcribe_function\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "    \n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "torch_dtype = torch.float32\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight.to(x.dtype),\n",
    "            None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )\n",
    "\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )\n",
    "\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def disable_sdpa():\n",
    "    prev_state = MultiHeadAttention.use_sdpa\n",
    "    try:\n",
    "        MultiHeadAttention.use_sdpa = False\n",
    "        yield\n",
    "    finally:\n",
    "        MultiHeadAttention.use_sdpa = prev_state\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    use_sdpa = False\n",
    "\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        q = self.query(x)\n",
    "\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "            # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n",
    "            a = scaled_dot_product_attention(\n",
    "                q, k, v, is_causal=mask is not None and n_ctx > 1\n",
    "            )\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "        return out, qk\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\n",
    "            the mel spectrogram of the audio\n",
    "        \"\"\"\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        print(f\"Encoder : {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [\n",
    "                ResidualAttentionBlock(n_state, n_head, cross_attention=True)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "        self.ln = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n",
    "            the text tokens\n",
    "        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n",
    "            the encoded audio features to be attended on\n",
    "        \"\"\"\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        x = (\n",
    "            self.token_embedding(x)\n",
    "            + self.positional_embedding[offset : offset + x.shape[-1]]\n",
    "        )\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (\n",
    "            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)\n",
    "        ).float()\n",
    "        print(f\"Decoder : {logits.shape}\")\n",
    "        return logits\n",
    "\n",
    "class Whisper(nn.Module):\n",
    "    def __init__(self, dims: ModelDimensions):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.dims.n_mels,\n",
    "            self.dims.n_audio_ctx,\n",
    "            self.dims.n_audio_state,\n",
    "            self.dims.n_audio_head,\n",
    "            self.dims.n_audio_layer,\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.dims.n_vocab,\n",
    "            self.dims.n_text_ctx,\n",
    "            self.dims.n_text_state,\n",
    "            self.dims.n_text_head,\n",
    "            self.dims.n_text_layer,\n",
    "        )\n",
    "        # use the last half among the decoder layers for time alignment by default;\n",
    "        # to use a specific set of heads, see `set_alignment_heads()` below.\n",
    "        all_heads = torch.zeros(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.dims.n_text_layer // 2 :] = True\n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head\n",
    "        )\n",
    "        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel: torch.Tensor):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):\n",
    "        return self.decoder(tokens, audio_features)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id):\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone() \n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features, labels=None, dec_input_ids=None):\n",
    "        if labels is not None:\n",
    "            if dec_input_ids is None:\n",
    "                dec_input_ids = self.shift_tokens_right(\n",
    "                    labels, self.dims.pad_token_id, self.dims.decoder_start_token_id\n",
    "                )\n",
    "\n",
    "        encoded_features = self.encoder(input_features).to(self.device)  \n",
    "        logits = self.decoder(dec_input_ids, encoded_features)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            labels = labels.to(logits.device).long()\n",
    "            loss = loss_fct(logits.view(-1, self.dims.n_vocab), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "     \n",
    "    def _initialize_weights(self, module):\n",
    "            nn.init.normal_(self.decoder.token_embedding.weight, mean=0.0, std=0.02)\n",
    "            if hasattr(self.decoder.positional_embedding, 'weight'):\n",
    "                nn.init.normal_(self.decoder.positional_embedding, mean=0.0, std=0.02)\n",
    "            for block in self.decoder.blocks:\n",
    "                for layer in block.children():\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        nn.init.xavier_normal_(layer.weight)\n",
    "                        if layer.bias is not None:\n",
    "                            nn.init.zeros_(layer.bias)\n",
    "\n",
    "            nn.init.constant_(self.decoder.ln.weight, 1)\n",
    "            if self.decoder.ln.bias is not None:\n",
    "                nn.init.constant_(self.decoder.ln.bias, 0)\n",
    "\n",
    "            nn.init.xavier_normal_(self.encoder.conv1.weight)\n",
    "            if self.encoder.conv1.bias is not None:\n",
    "                nn.init.zeros_(self.encoder.conv1.bias)\n",
    "\n",
    "            nn.init.kaiming_normal_(self.encoder.conv2.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if self.encoder.conv2.bias is not None:\n",
    "                nn.init.zeros_(self.encoder.conv2.bias)\n",
    "\n",
    "            nn.init.constant_(self.encoder.ln_post.weight, 1)\n",
    "            if self.encoder.ln_post.bias is not None:\n",
    "                nn.init.constant_(self.encoder.ln_post.bias, 0)\n",
    "\n",
    "    def apply_initialization(self, module):\n",
    "        self._initialize_weights( module )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.dims.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        The `MultiHeadAttention` module optionally accepts `kv_cache` which stores the key and value\n",
    "        tensors calculated for the previous positions. This method returns a dictionary that stores\n",
    "        all caches, and the necessary hooks for the key and value projection modules that save the\n",
    "        intermediate tensors to be reused during later calculations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cache : Dict[nn.Module, torch.Tensor]\n",
    "            A dictionary object mapping the key/value projection modules to its cache\n",
    "        hooks : List[RemovableHandle]\n",
    "            List of PyTorch RemovableHandle objects to stop the hooks to be called\n",
    "        \"\"\"\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.dims.n_text_ctx:\n",
    "                # save as-is, for the first token or cross attention\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n",
    "    pad_token_id: int\n",
    "    bos_token_id: int\n",
    "    decoder_start_token_id: int\n",
    "    eos_token_id: int\n",
    "    pad_token_id: int\n",
    "    init_std: float\n",
    "\n",
    "dims = ModelDimensions(\n",
    "    bos_token_id=50257,\n",
    "    decoder_start_token_id=50258,\n",
    "    eos_token_id=50257,\n",
    "    init_std=0.02,\n",
    "    n_audio_ctx=1500,\n",
    "    n_audio_head=16,\n",
    "    n_audio_layer=24,\n",
    "    n_audio_state=1024,\n",
    "    n_mels=128,\n",
    "    n_text_ctx=448,\n",
    "    n_text_head=16,\n",
    "    n_text_layer=16,\n",
    "    n_text_state=1024,\n",
    "    pad_token_id=50257,\n",
    "    n_vocab=51865,\n",
    ")\n",
    "\n",
    "model = Whisper(dims).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model_name_or_path=\"openai/whisper-small\", feature_size=128)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(pretrained_model_name_or_path=\"openai/whisper-small\", language=\"en\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(pretrained_model_name_or_path=\"openai/whisper-small\")\n",
    "\n",
    "class GradientClippingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=kwargs[\"model\"].parameters(), max_norm=0.98)\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self, tb_writer, tokenizer, metric, log_every_n_steps=1):\n",
    "        super().__init__()\n",
    "        self.tb_writer = tb_writer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metric = metric\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self.predictions = None\n",
    "        self.label_ids = None\n",
    "\n",
    "    def compute_wer(self, pred_str, label_str):\n",
    "        wer = 100 * self.metric.compute(predictions=pred_str, references=label_str)\n",
    "        return wer\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model, metrics=None, **kwargs):\n",
    "        if metrics is not None:\n",
    "            self.eval_loss = metrics.get('eval_loss')\n",
    "\n",
    "            if state.global_step % self.log_every_n_steps == 0:\n",
    "                for key, value in metrics.items():\n",
    "                    if key.startswith(\"eval_\"):\n",
    "                        self.tb_writer.add_scalar(key, value, state.global_step)\n",
    "\n",
    "        if self.predictions is not None and self.label_ids is not None:\n",
    "            pred_str = self.tokenizer.batch_decode(self.predictions, skip_special_tokens=True)\n",
    "            label_str = self.tokenizer.batch_decode(self.label_ids, skip_special_tokens=True)\n",
    "\n",
    "            if state.global_step % self.log_every_n_steps == 0:\n",
    "                sample_index = 0\n",
    "                self.tb_writer.add_text(f\"Prediction\", pred_str[sample_index], state.global_step)\n",
    "                self.tb_writer.add_text(f\"Label\", label_str[sample_index], state.global_step)\n",
    "                print(f\"Evaluation: - Step {state.global_step} - Loss: {self.eval_loss:.4f}\")\n",
    "                print(f\"Prediction: {pred_str[sample_index]}\")\n",
    "                print(f\"Label: {label_str[sample_index]}\")\n",
    "                print(\"-\" * 10)\n",
    "\n",
    "        self.predictions = None\n",
    "        self.label_ids = None\n",
    "\n",
    "def create_compute_metrics(callback_instance):\n",
    "    def compute_metrics(eval_pred):\n",
    "        pred_logits = eval_pred.predictions\n",
    "        label_ids = eval_pred.label_ids\n",
    "\n",
    "        if isinstance(pred_logits, tuple):\n",
    "            pred_ids = pred_logits[0]\n",
    "        else:\n",
    "            pred_ids = pred_logits\n",
    "        if pred_ids.ndim == 3:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "        label_ids[label_ids == -100] = callback_instance.tokenizer.pad_token_id\n",
    "        callback_instance.predictions = pred_ids\n",
    "        callback_instance.label_ids = label_ids\n",
    "\n",
    "        pred_str = callback_instance.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = callback_instance.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "        wer = 100 * callback_instance.metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "        pred_flat = pred_ids.flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        mask = labels_flat != callback_instance.tokenizer.pad_token_id\n",
    "\n",
    "        accuracy = accuracy_score(y_true=labels_flat[mask], y_pred=pred_flat[mask])\n",
    "        precision = precision_score(y_true=labels_flat[mask], y_pred=pred_flat[mask], average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true=labels_flat[mask], y_pred=pred_flat[mask], average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true=labels_flat[mask], y_pred=pred_flat[mask], average='weighted', zero_division=0)\n",
    "\n",
    "        return {\"wer\": wer, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    return compute_metrics\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    tokenizer: Any\n",
    "    feature_extractor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def get_length_of_dataset(dataset):\n",
    "    length = 0\n",
    "    for item in dataset:\n",
    "        length += len(item[\"audio\"][\"array\"]) / item[\"audio\"][\"sampling_rate\"]\n",
    "    return length / 3600\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    batch[\"input_features\"] = feature_extractor(batch[\"audio\"][\"array\"], sampling_rate=batch[\"audio\"][\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "train = load_dataset(\"fixie-ai/librispeech_asr\", \"clean\", split=\"train.100\", streaming=True, trust_remote_code=True).map(prepare_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "\n",
    "test = load_dataset(\"fixie-ai/librispeech_asr\", \"clean\", split=\"test\", streaming=True, trust_remote_code=True).map(prepare_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "log_dir = \"D:/project/logs\"\n",
    "metric = evaluate.load(path=\"wer\")\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "metrics_callback = MetricsCallback(tb_writer=tb_writer, tokenizer=tokenizer, metric=metric, log_every_n_steps=20)\n",
    "compute_metrics = create_compute_metrics(callback_instance=metrics_callback)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=log_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    warmup_steps = 100,\n",
    "    max_steps = 1000,\n",
    "    save_steps = 1000,\n",
    "    eval_steps = 50,\n",
    "    logging_steps = 5,\n",
    "    logging_dir=log_dir + \"/logs_hf\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adafactor\",\n",
    "    weight_decay=0.0025,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"steps\",\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    "    eval_on_start=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    callbacks=[metrics_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "eval_results = trainer.evaluate()\n",
    "# model.save_pretrained(log_dir+name+\"_b\", safe_serialization=False)\n",
    "# import tensorboard\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
