{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import functools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import aiohttp\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, amp, optim, nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from threading import Thread\n",
    "from typing import Dict, Optional, Tuple, Union, List, Any\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, PretrainedConfig, TrainerCallback,\n",
    "    WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizerFast\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from evaluate import module\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, IterableDatasetDict, Audio\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "torch.set_default_dtype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, initial_value, learnable=True):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        if learnable:\n",
    "            self.value = nn.Parameter(torch.tensor(initial_value))\n",
    "        else:\n",
    "            self.register_buffer('value', torch.tensor(initial_value))\n",
    "    def forward(self):\n",
    "        return self.value\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor: # type: ignore\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight.to(x.dtype),\n",
    "            None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward( # type: ignore\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor: # type: ignore\n",
    "        return super().forward(x.float()).type(x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CombinedRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, base, n_state, n_head):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        assert self.n_state % self.n_head == 0, \"n_state must be divisible by n_head\"\n",
    "        self.h_dim = self.n_state // self.n_head\n",
    "        assert self.h_dim % 2 == 0, \"Head dimension must be even for rotary embeddings\"\n",
    "        self.num_rotations = ((n_state // n_head) // 2)\n",
    "        \n",
    "        self.thetas = nn.Parameter(torch.zeros(self.num_rotations)) \n",
    "        self.rotation_pairs = nn.Parameter(data=torch.rand(self.num_rotations, 2) * self.h_dim)\n",
    "        self.theta_scale = nn.Parameter(data=torch.ones(1))\n",
    "        self.rotation_matrix = nn.Parameter(data=torch.eye(n=self.h_dim))\n",
    "        self.inv_freq = nn.Parameter(data=1.0 / (self.base ** (torch.arange(start=0, end=self.h_dim, step=2).float() / self.h_dim)))\n",
    "        self.num_rotations_scale = nn.Parameter(data=torch.ones(1))\n",
    "\n",
    "    def givens_rotation_matrix(self, n_state, i, j, theta):\n",
    "        G = torch.eye(n_state).to(device)\n",
    "        G[i, i] = math.cos(theta)\n",
    "        G[i, j] = -math.sin(theta)\n",
    "        G[j, i] = math.sin(theta)\n",
    "        G[j, j] = math.cos(theta)\n",
    "        return G\n",
    "        \n",
    "    def update_base(self, new_base):\n",
    "        if new_base is not None and new_base != self.base:\n",
    "            self.base = new_base\n",
    "            inv_freq = 1.0 / (self.base ** (torch.arange(start=0, end=self.h_dim, step=2).float() / self.h_dim))\n",
    "            self.inv_freq.data.copy_(inv_freq)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(tensor=self.rotation_matrix)\n",
    "        nn.init.zeros_(tensor=self.thetas)\n",
    "\n",
    "    def forward(self, x, new_base=None):\n",
    "        self.update_base(new_base) \n",
    "\n",
    "        if x.dim() not in [3, 4]:\n",
    "            raise ValueError(f\"Expected input tensor to be 3D or 4D, but got {x.dim()}D\")\n",
    "\n",
    "        batch_size, seq_len, *rest = x.size()\n",
    "\n",
    "        if x.dim() == 3:\n",
    "            n_state = rest[0]\n",
    "            if n_state != self.n_head * self.h_dim:\n",
    "                raise ValueError(f\"Expected n_state ({n_state}) to be compatible with n_head ({self.n_head}) * h_dim ({self.h_dim} = {self.n_head * self.h_dim})\") \n",
    "        else: \n",
    "            n_head, h_dim = rest\n",
    "            if n_head != self.n_head or h_dim != self.h_dim:\n",
    "                raise ValueError(f\"For 4D input, expected n_head {self.n_head} and h_dim {self.h_dim}, but got n_head {n_head} and h_dim {h_dim}\")\n",
    "\n",
    "        x = x.view(batch_size, seq_len, self.n_head, self.h_dim) \n",
    "        x = x.reshape(-1, self.h_dim)\n",
    "        adjusted_num_rotations = int(torch.round(self.num_rotations * self.num_rotations_scale))\n",
    "\n",
    "        for k in range(adjusted_num_rotations):\n",
    "            i, j = self.rotation_pairs[k].long()\n",
    "            theta = self.thetas[k] * self.theta_scale\n",
    "            G = self.givens_rotation_matrix(n_state=self.h_dim, i=i, j=j, theta=theta).to(device)\n",
    "            x = torch.matmul(input=x, other=G).to(device)\n",
    "\n",
    "        x = torch.matmul(input=x, other=self.rotation_matrix)\n",
    "        x = x.view(batch_size, seq_len, self.n_head, self.h_dim)\n",
    "\n",
    "        sinusoid_inp = torch.einsum('i, j -> i j', torch.arange(end=seq_len, device=x.device), self.inv_freq.to(device=x.device))\n",
    "        sin = sinusoid_inp.sin()[None, :, None, :]\n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x = torch.cat(tensors=[x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        x = x.view(batch_size, seq_len, self.n_state)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LearnedSinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, n_ctx, n_state, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "        position = torch.arange(start=0, end=self.n_ctx, dtype=torch.float).unsqueeze(dim=1)\n",
    "        div_term = torch.exp(input=torch.arange(start=0, end=self.n_state, step=2).float() * -(math.log(10000.0) / self.n_state))\n",
    "        features = torch.zeros(self.n_ctx, self.n_state)\n",
    "        features[:, 0::2] = torch.sin(input=position * div_term)\n",
    "        features[:, 1::2] = torch.cos(input=position * div_term)\n",
    "        self.register_buffer('my_big_toe', tensor=features)\n",
    "        self.positional_embeddings = nn.Parameter(self.my_big_toe.clone())\n",
    "\n",
    "    def forward(self, positions):\n",
    "        position_embeddings = self.positional_embeddings[positions]\n",
    "        return position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, base, n_state, n_head, max_dist):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "        assert self.n_state % self.n_head == 0, \"n_state must be divisible by n_head\"\n",
    "        self.h_dim = self.n_state // self.n_head\n",
    "        assert self.h_dim % 2 == 0, \"Head dimension must be even for rotary embeddings\"\n",
    "\n",
    "        self.query = nn.Linear(self.n_state, self.n_state)\n",
    "        self.key = nn.Linear(self.n_state, self.n_state, bias=False)\n",
    "        self.value = nn.Linear(self.n_state, self.n_state)\n",
    "        self.out = nn.Linear(self.n_state, self.n_state)\n",
    "\n",
    "        self.combined_rotary = CombinedRotaryEmbedding(base, n_state, n_head)\n",
    "        self.kv_cache = {}\n",
    "        \n",
    "        self.positional_scaling = nn.Parameter(torch.ones(1))\n",
    "        self.rel_pos_bias = nn.Parameter(torch.zeros((2 * int(self.max_dist) - 1, self.n_head))).to(device)\n",
    "        self.inv_freq = nn.Parameter(data=1.0 / (self.base ** (torch.arange(start=0, end=self.h_dim, step=2).float() / self.h_dim)))\n",
    "\n",
    "    def update_base(self, new_base):\n",
    "        if new_base is not None and new_base != self.base:\n",
    "            self.base = new_base\n",
    "            inv_freq = 1.0 / (self.base ** (torch.arange(start=0, end=self.h_dim, step=2).float() / self.h_dim))\n",
    "            self.inv_freq.data.copy_(inv_freq)\n",
    "            self.combined_rotary.update_base(self.base)\n",
    "\n",
    "    def update_dist(self, new_dist):\n",
    "        if new_dist is not None and new_dist != new_dist:\n",
    "            self.max_dist = new_dist\n",
    "            rel_pos_bias = nn.Parameter(torch.zeros((2 * int(self.max_dist) - 1, self.n_head)))\n",
    "            self.rel_pos_bias.data.copy_(rel_pos_bias)\n",
    "        \n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, \n",
    "                mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None, new_dist=None, new_base=None):\n",
    "        \n",
    "        self.update_base(new_base) \n",
    "        self.update_dist(new_dist)\n",
    "\n",
    "        q = self.query(x)\n",
    "\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = self.combined_rotary(q) * self.positional_scaling\n",
    "        k = self.combined_rotary(k) * self.positional_scaling\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "   \n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tuple     [torch.Tensor, Optional[torch.Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        seq_len_q = q.size(2)\n",
    "        seq_len_k = k.size(2)\n",
    "        positions = torch.arange(end=seq_len_q, device=q.device).unsqueeze(dim=1) - torch.arange(end=seq_len_k, device=q.device).unsqueeze(dim=0)\n",
    "        positions = positions.clamp(min=-self.max_dist + 1, max=self.max_dist - 1) + self.max_dist - 1\n",
    "        rel_bias = self.rel_pos_bias[positions]\n",
    "        rel_bias = rel_bias.permute(2, 0, 1).unsqueeze(0)  \n",
    "\n",
    "        \n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale \n",
    "        attn_scores = attn_scores + rel_bias\n",
    "\n",
    "        a = scaled_dot_product_attention(q, k, v, attn_mask = attn_scores , is_causal=mask is not None and n_ctx > 1) \n",
    "        out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "        attn_scores = attn_scores.reshape(n_batch, n_ctx, -1)[:,:,:self.n_state]\n",
    "\n",
    "        return out, attn_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, base: int, n_state: int, n_head: int, max_dist: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiheadAttention(base, n_state, n_head, max_dist)\n",
    "        self.attn_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, n_mlp), nn.GELU(), nn.Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, base, n_mels: int, n_state: int, n_head: int, n_layer: int, n_ctx: int, max_dist: int):\n",
    "        super().__init__()\n",
    "        self.h_dim = n_state // n_head\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.combined_rotary = CombinedRotaryEmbedding(base, n_state,  n_head)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(base, n_state, n_head, max_dist) for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def update_base(self, new_base):\n",
    "        if new_base is not None:\n",
    "            self.combined_rotary.update_base(new_base)\n",
    "\n",
    "    def forward(self, x, new_base=None):\n",
    "        self.update_base(new_base)\n",
    "        x = F.gelu(input=self.conv1(x))\n",
    "        x = F.gelu(input=self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.combined_rotary(x) \n",
    "        for block in self.blocks:\n",
    "                  x = block(x)\n",
    "        x = self.ln_post(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, base, n_vocab, n_state, n_head, n_layer, n_ctx, max_dist):  \n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(base, n_state, n_head, max_dist) for _ in range(n_layer)])\n",
    "        \n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "        self.ln = LayerNorm(n_state)\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(value=-np.inf).triu_(diagonal=1)\n",
    "        self.register_buffer(name=\"mask\", tensor=mask, persistent=False)\n",
    "\n",
    "    def forward(self, x, xa, kv_cache=None, new_base=None):\n",
    "\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        positions = torch.arange(end=x.shape[1], device=x.device) + offset\n",
    "        pos = self.positional_embedding(positions).unsqueeze(0)\n",
    "        tok = self.token_embedding(x)\n",
    "        # rot = self.combined_rotary(x)\n",
    "        x = pos + tok\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, kv_cache)\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(dtype=x.dtype), 0, 1)).float()\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EchoConfig(PretrainedConfig):\n",
    "    model_type = \"Echo\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        base=10000,\n",
    "        bos_token_id=50257,\n",
    "        decoder_start_token_id=50258,\n",
    "        eos_token_id=50257,\n",
    "        init_std=0.03,\n",
    "        max_dist=128,\n",
    "        n_audio_ctx=1500,\n",
    "        n_audio_head=16,\n",
    "        n_audio_layer=24,\n",
    "        n_audio_state=1024,\n",
    "        n_mels=128,\n",
    "        n_text_ctx=448,\n",
    "        n_text_head=16,\n",
    "        n_text_layer=16,\n",
    "        n_text_state=1024,\n",
    "        pad_token_id=50257,\n",
    "        unk_token_id=50257,\n",
    "        n_vocab=51865,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        super(EchoConfig, self).__init__(**kwargs)\n",
    "        self.base = base\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.decoder_start_token_id = decoder_start_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.init_std = init_std\n",
    "        self.max_dist = max_dist\n",
    "        self.n_audio_ctx = n_audio_ctx\n",
    "        self.n_audio_head = n_audio_head\n",
    "        self.n_audio_layer = n_audio_layer\n",
    "        self.n_audio_state = n_audio_state\n",
    "        self.n_mels = n_mels\n",
    "        self.n_text_ctx = n_text_ctx\n",
    "        self.n_text_head = n_text_head\n",
    "        self.n_text_layer = n_text_layer\n",
    "        self.n_text_state = n_text_state\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.unk_token_id = unk_token_id\n",
    "        self.n_vocab = n_vocab\n",
    "\n",
    "class Echo(PreTrainedModel):\n",
    "    config_class = EchoConfig\n",
    "\n",
    "    def __init__(self, config: EchoConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = AudioEncoder(\n",
    "            base=self.config.base,\n",
    "            n_state=self.config.n_audio_state, \n",
    "            n_head=self.config.n_audio_head,\n",
    "            n_mels=self.config.n_mels,\n",
    "            n_layer=self.config.n_audio_layer,\n",
    "            n_ctx=self.config.n_audio_ctx,\n",
    "            max_dist=self.config.max_dist,\n",
    "        )\n",
    "\n",
    "        self.decoder = TextDecoder(\n",
    "            base=self.config.base,\n",
    "            n_state=self.config.n_text_state, \n",
    "            n_head=self.config.n_text_head,\n",
    "            n_vocab=self.config.n_vocab,\n",
    "            n_layer=self.config.n_text_layer,\n",
    "            n_ctx=self.config.n_text_ctx,\n",
    "            max_dist=self.config.max_dist,\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        all_heads = torch.zeros(self.config.n_text_layer, self.config.n_text_head, dtype=torch.bool) \n",
    "        all_heads[self.config.n_text_layer // 2:] = True \n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "        self.base = self.config.base\n",
    "        self.max_dist= self.config.max_dist\n",
    "        self.adjust_counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.kv_cache = {}\n",
    "\n",
    "    def update_dist(self, new_dist):\n",
    "        self.new_dist = new_dist\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            if isinstance(module, MultiheadAttention):\n",
    "                module.update_dist(self.new_dist)\n",
    "        for name, module in self.decoder.named_modules():\n",
    "            if isinstance(module, MultiheadAttention):\n",
    "                module.update_dist(self.new_dist)\n",
    "\n",
    "    def adjust_max_dist(self, loss, step_size=1, threshold=0.0005):\n",
    "        if self.adjust_counter % 25 == 0:\n",
    "            if loss < self.best_loss:\n",
    "                potential_new_dist = self.max_dist + step_size\n",
    "            else:\n",
    "                potential_new_dist = max(1, self.max_dist - step_size)\n",
    "            if abs(potential_new_dist - self.max_dist) >= threshold:\n",
    "                new_dist = potential_new_dist\n",
    "                self.update_dist(new_dist)\n",
    "                self.max_dist = new_dist\n",
    "                self.best_loss = loss\n",
    "\n",
    "        self.adjust_counter += 1\n",
    "        return self.max_dist\n",
    "\n",
    "    def adjust_base(self, loss, factor=1.0025):\n",
    "        if self.adjust_counter % 25 == 0:\n",
    "            if loss < self.best_loss:\n",
    "                new_base = self.base * factor\n",
    "            else:\n",
    "                new_base = self.base / factor\n",
    "            self.update_base(new_base)\n",
    "            self.base = new_base\n",
    "            self.best_loss = loss\n",
    "   \n",
    "        self.adjust_counter += 1\n",
    "        return self.base\n",
    "\n",
    "    def update_base(self, new_base):\n",
    "        self.new_base=new_base\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            if isinstance(module, (MultiheadAttention, CombinedRotaryEmbedding, AudioEncoder)):\n",
    "                module.update_base(self.new_base)\n",
    "        for name, module in self.decoder.named_modules():\n",
    "            if isinstance(module, (MultiheadAttention, CombinedRotaryEmbedding)):\n",
    "                module.update_base(self.new_base)\n",
    "\n",
    "    def print_update(self):\n",
    "        self.adjust_counter += 1\n",
    "        if self.adjust_counter % 20 == 0:\n",
    "            print(f\"{self.adjust_counter}: Loss: {self.best_loss}  Base: {self.base}, Distance: {self.max_dist}\")\n",
    "            \n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id):\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone() \n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features, labels=None, dec_input_ids=None):\n",
    "        if labels is not None:\n",
    "            if dec_input_ids is None:\n",
    "                dec_input_ids = self.shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "\n",
    "        encoded_features = self.encoder(input_features).to(self.device)  \n",
    "        logits = self.decoder(dec_input_ids, encoded_features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            labels = labels.to(logits.device).long()\n",
    "            loss = loss_fct(logits.view(-1, self.config.n_vocab), labels.view(-1))\n",
    "\n",
    "            self.adjust_base(loss.item())\n",
    "            self.adjust_max_dist(loss.item())\n",
    "            self.print_update()  \n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "###\n",
    "    def reset_parameters(self):\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            if isinstance(module, CombinedRotaryEmbedding):\n",
    "                module.reset_parameters()\n",
    "        self.encoder.apply(self._init_weights)\n",
    "        \n",
    "    def _initialize_weights(self, module):\n",
    "            nn.init.normal_(self.decoder.token_embedding.weight, mean=0.0, std=self.config.init_std)\n",
    "            if hasattr(self.decoder.positional_embedding, 'weight'):\n",
    "                nn.init.normal_(self.decoder.positional_embedding.weight, mean=0.0, std=self.config.init_std)\n",
    "            for block in self.decoder.blocks:\n",
    "                for layer in block.children():\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        nn.init.xavier_normal_(layer.weight)\n",
    "                        if layer.bias is not None:\n",
    "                            nn.init.zeros_(layer.bias)\n",
    "\n",
    "            nn.init.constant_(self.decoder.ln.weight, 1)\n",
    "            if self.decoder.ln.bias is not None:\n",
    "                nn.init.constant_(self.decoder.ln.bias, 0)\n",
    "\n",
    "            nn.init.xavier_normal_(self.encoder.conv1.weight)\n",
    "            if self.encoder.conv1.bias is not None:\n",
    "                nn.init.zeros_(self.encoder.conv1.bias)\n",
    "\n",
    "            nn.init.kaiming_normal_(self.encoder.conv2.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if self.encoder.conv2.bias is not None:\n",
    "                nn.init.zeros_(self.encoder.conv2.bias)\n",
    "\n",
    "            nn.init.constant_(self.encoder.ln_post.weight, 1)\n",
    "            if self.encoder.ln_post.bias is not None:\n",
    "                nn.init.constant_(self.encoder.ln_post.bias, 0)\n",
    "\n",
    "    def apply_initialization(self, module):\n",
    "        self._initialize_weights( module )\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.config.n_text_layer, self.config.n_text_head\n",
    "        )\n",
    "        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, labels, input_features):\n",
    "        return self.decoder(labels, input_features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def supports_gradient_checkpointing(self):\n",
    "        return True\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiheadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_features': input_ids}\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(self, batch_size, decoder_start_token_id=None, bos_token_id=None):\n",
    "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * decoder_start_token_id\n",
    "\n",
    "    def can_generate(self):\n",
    "        return True\n",
    "\n",
    "    def generate(self, inputs, **kwargs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_input_ids = torch.zeros((inputs.size(0), 1), dtype=torch.long, device=inputs.device)\n",
    "        outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        return outputs.argmax(dim=-1)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, enable=True, gradient_checkpointing_func=checkpoint):\n",
    "        self.checkpointing = enable\n",
    "        self.gradient_checkpointing_func = gradient_checkpointing_func\n",
    "        for module in self.modules():\n",
    "            if hasattr(module, 'checkpointing'):\n",
    "                module.checkpointing = enable\n",
    "                module.gradient_checkpointing_func = gradient_checkpointing_func\n",
    "\n",
    "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
    "        if not self.supports_gradient_checkpointing:\n",
    "            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")\n",
    "        if gradient_checkpointing_kwargs is None:\n",
    "            gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "        gradient_checkpointing_func = functools.partial(checkpoint, **gradient_checkpointing_kwargs)\n",
    "        self._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=gradient_checkpointing_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = EchoConfig(\n",
    "    base=10000,\n",
    "    bos_token_id=50257,\n",
    "    decoder_start_token_id=50258,\n",
    "    eos_token_id=50257,\n",
    "    init_std=0.02,\n",
    "    max_dist=128,\n",
    "    n_audio_ctx=1500,\n",
    "    n_audio_head=16,\n",
    "    n_audio_layer=20,\n",
    "    n_audio_state=1024,\n",
    "    n_mels=128,\n",
    "    n_text_ctx=448,\n",
    "    n_text_head=16,\n",
    "    n_text_layer=16,\n",
    "    n_text_state=1024,\n",
    "    pad_token_id=50257,\n",
    "    unk_token_id=50257,\n",
    "    n_vocab=51865,\n",
    "    )\n",
    "\n",
    "model = Echo(config=config).to('cuda')\n",
    "model.apply_initialization(module)\n",
    "\n",
    "name=\"./echo/\"\n",
    "config.save_pretrained(name)\n",
    "model.save_pretrained(name)\n",
    "torch.save(model.state_dict(), name+\"state_dict.pt\")\n",
    "model = Echo.from_pretrained(name).to('cuda')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
