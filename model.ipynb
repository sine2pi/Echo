{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64, os, evaluate, random, gzip, math, torch, numpy as np, torch.nn.functional as F, json, warnings\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from datasets import load_dataset, IterableDatasetDict, Audio\n",
    "from transformers import (Seq2SeqTrainer, Seq2SeqTrainingArguments, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizerFast)\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from itertools import chain\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from typing import Dict, Optional, Tuple\n",
    "from torch import Tensor, nn\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Tuple, Union, List, Any\n",
    "from safetensors.torch import save_file, load_file\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "device = torch.device(device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "torch.set_default_dtype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dimensions:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: int,\n",
    "        text_ctx: int,\n",
    "        text_state: int,\n",
    "        text_head: int,\n",
    "        text_layerA: int,\n",
    "        text_layerB: int,\n",
    "        audio_ctx: int,\n",
    "        audio_state: int,\n",
    "        audio_head: int,\n",
    "        audio_layerA: int,\n",
    "        audio_layerB: int,\n",
    "        mels: int,\n",
    "        checkpoint: bool,\n",
    "        dropout: float,\n",
    "        activation: str,\n",
    "    ):\n",
    "        self.vocab = vocab\n",
    "        self.text_ctx = text_ctx\n",
    "        self.text_state = text_state\n",
    "        self.text_head = text_head\n",
    "        self.text_layerA = text_layerA\n",
    "        self.text_layerB = text_layerB\n",
    "        self.audio_ctx = audio_ctx\n",
    "        self.audio_state = audio_state\n",
    "        self.audio_head = audio_head\n",
    "        self.audio_layerA = audio_layerA\n",
    "        self.audio_layerB = audio_layerB\n",
    "        self.mels = mels\n",
    "        self.checkpoint = checkpoint\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config: dict):\n",
    "        return cls(\n",
    "            vocab=config.get(\"vocab_size\", 51865),\n",
    "            text_ctx=config.get(\"text_ctx\", 448),\n",
    "            text_state=config.get(\"hidden_size\", 768),\n",
    "            text_head=config.get(\"num_attention_heads\", 12),\n",
    "            text_layerA=config.get(\"num_hidden_layers\", 12),\n",
    "            text_layerB=config.get(\"text_layerB\", 0),\n",
    "            audio_ctx=config.get(\"audio_ctx\", 1500),\n",
    "            audio_state=config.get(\"audio_state\", 768),\n",
    "            audio_head=config.get(\"audio_head\", 12),\n",
    "            audio_layerA=config.get(\"num_encoder_layers\", 12),\n",
    "            audio_layerB=config.get(\"audio_layerB\", 0),\n",
    "            mels=config.get(\"mels\", 80),\n",
    "            checkpoint=config.get(\"checkpoint\", False),\n",
    "            dropout=config.get(\"dropout\", 0.01),\n",
    "            activation=config.get(\"activation\", \"gelu\"))\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"vocab_size\": self.vocab,\n",
    "            \"text_ctx\": self.text_ctx,\n",
    "            \"hidden_size\": self.text_state,\n",
    "            \"num_attention_heads\": self.text_head,\n",
    "            \"num_hidden_layers\": self.text_layerA,\n",
    "            \"audio_ctx\": self.audio_ctx,\n",
    "            \"audio_state\": self.audio_state,\n",
    "            \"audio_head\": self.audio_head,\n",
    "            \"num_encoder_layers\": self.audio_layerA,\n",
    "            \"mels\": self.mels,\n",
    "            \"checkpoint\": self.checkpoint,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"activation\": self.activation,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight.to(x.dtype),\n",
    "            None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class rotary(nn.Module):\n",
    "    def __init__(self, ctx, dims, heads, base=10000, theta_learnable=False, rot_learnable=False,\n",
    "                 matrix_learnable=False, freq_learnable=False):\n",
    "        super().__init__()\n",
    "        self.ctx = ctx\n",
    "        self.dims = dims\n",
    "        self.heads = heads\n",
    "        self.base = base\n",
    "\n",
    "        self.head_dim = self.dims // self.heads\n",
    "        self.rot = self.head_dim // 2\n",
    "\n",
    "        self.thetas = nn.Parameter(torch.zeros(self.rot))\n",
    "        self.r_pairs = nn.Parameter(torch.rand(self.rot, 2) * self.head_dim)\n",
    "        self.theta_scale = nn.Parameter(torch.ones(1), requires_grad=theta_learnable)\n",
    "        self.rot_scale = nn.Parameter(torch.ones(1), requires_grad=rot_learnable)\n",
    "        self.r_matrix = nn.Parameter(torch.eye(self.head_dim), requires_grad=matrix_learnable)\n",
    "\n",
    "        freq_data = 1.0 / (self.base ** (torch.arange(0, self.head_dim, 2).float() / self.head_dim))\n",
    "        self.inv_freq = nn.Parameter(freq_data, requires_grad=freq_learnable)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.r_matrix)\n",
    "        nn.init.zeros_(self.thetas)\n",
    "\n",
    "    def q_rotation(self, x, theta, u, v):\n",
    "        u = u / torch.norm(u)\n",
    "        v = v / torch.norm(v)\n",
    "\n",
    "        half_theta = theta / 2\n",
    "        cos_ht = torch.cos(half_theta)\n",
    "        sin_ht = torch.sin(half_theta)\n",
    "\n",
    "        q = torch.cat([cos_ht.unsqueeze(0), sin_ht * u])\n",
    "        q_conj = torch.cat([cos_ht.unsqueeze(0), -sin_ht * u])\n",
    "\n",
    "        x_shape = x.shape\n",
    "        x = x.view(-1, 3)\n",
    "\n",
    "        uv_cross = torch.cross(u.unsqueeze(0), x)\n",
    "        uuv_cross = torch.cross(u.unsqueeze(0), uv_cross)\n",
    "        x_rot = x + 2 * (q[0] * uv_cross + uuv_cross)\n",
    "\n",
    "        x_rot = x_rot.view(*x_shape)\n",
    "        return x_rot\n",
    "\n",
    "    def rotation_matrix(self, dims, i, j, theta):\n",
    "        G = torch.eye(dims, device=theta.device)\n",
    "        c, s = torch.cos(theta), torch.sin(theta)\n",
    "        G[i, i], G[j, j] = c, c\n",
    "        G[i, j], G[j, i] = -s, s\n",
    "\n",
    "        if dims == 3:\n",
    "            u = torch.eye(dims, device=theta.device)[i]\n",
    "            v = torch.eye(dims, device=theta.device)[j]\n",
    "            Q = self.q_rotation(torch.eye(dims, device=theta.device), theta=theta, u=u, v=v)\n",
    "            G = (G + Q) / 2\n",
    "        return G\n",
    "\n",
    "    def apply_rotations(self, x):\n",
    "        adjusted_rot = int(torch.round(self.rot_scale * self.rot))\n",
    "        for k in range(adjusted_rot):\n",
    "            i, j = self.r_pairs[k].long()\n",
    "            theta = self.thetas[k] * self.theta_scale\n",
    "            G = self.rotation_matrix(self.head_dim, i.item(), j.item(), theta)\n",
    "            x = x @ G\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, *rest = x.size()\n",
    "\n",
    "        if len(rest) == 1:\n",
    "            dims = rest[0]\n",
    "            if dims != self.heads * self.head_dim:\n",
    "                raise ValueError(f\"Needed {self.heads * self.head_dim}, but got too many {dims}\")\n",
    "        elif len(rest) == 2:\n",
    "            heads, head_dim = rest\n",
    "            if heads != self.heads or head_dim != self.head_dim:\n",
    "                raise ValueError(f\"This many heads {self.heads} and head_dims {self.head_dim} we need, got this many heads {heads} and head_dims {head_dim} we did.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Expected the thingy to be 3D or 4D, but got {x.dim()}D\")\n",
    "\n",
    "        x = x.view(batch_size, seq_len, self.heads, self.head_dim)\n",
    "        x = x.reshape(-1, self.head_dim)\n",
    "\n",
    "        x = self.apply_rotations(x)\n",
    "        x = x @ self.r_matrix\n",
    "\n",
    "        x = x.view(batch_size, seq_len, self.heads, self.head_dim)\n",
    "\n",
    "        position = torch.arange(seq_len, device=x.device, dtype=x.dtype).unsqueeze(1)\n",
    "        div_term = self.inv_freq.unsqueeze(0)\n",
    "        sinusoid_inp = position * div_term\n",
    "\n",
    "        sin = torch.sin(sinusoid_inp).unsqueeze(0).unsqueeze(2)\n",
    "        cos = torch.cos(sinusoid_inp).unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        x = x.view(batch_size, seq_len, self.dims)\n",
    "        x = x * math.sqrt(self.dims)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dims, ctx):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dims = dims\n",
    "        self.ctx = ctx\n",
    "        self.pe = self.get_positional_encoding(max_seq_len=ctx)\n",
    "        \n",
    "    def get_positional_encoding(self, max_seq_len):\n",
    "        pe = torch.zeros(max_seq_len, self.dims)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.dims, 2, dtype=torch.float32) * (-math.log(10000.0) / self.dims))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.pe[:, :seq_len, :]\n",
    "        x = x * math.sqrt(self.dims)\n",
    "        x = x + pe\n",
    "        return x\n",
    "    \n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiheadA(nn.Module):\n",
    "    use_sdpa: bool = True  \n",
    "    def __init__(self, dims: int, heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        if dims % heads != 0:\n",
    "            raise ValueError(f\"dims ({dims}) must be divisible by heads ({heads})\")\n",
    "        if not isinstance(dims, int) or not isinstance(heads, int):\n",
    "            raise TypeError(\"dims and heads must be integers\")\n",
    "            \n",
    "        self.heads = heads\n",
    "        self.dims = dims\n",
    "        self.head_dim = dims // heads\n",
    "        \n",
    "        self.query = Linear(in_features=dims, out_features=dims)\n",
    "        self.key = Linear(in_features=dims, out_features=dims, bias=False)\n",
    "        self.value = Linear(in_features=dims, out_features=dims)\n",
    "        self.out = Linear(in_features=dims, out_features=dims)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.query.weight, std=0.02)\n",
    "        nn.init.normal_(self.key.weight, std=0.02)\n",
    "        nn.init.normal_(self.value.weight, std=0.02)\n",
    "        nn.init.normal_(self.out.weight, std=0.02)\n",
    "        if self.query.bias is not None:\n",
    "            nn.init.zeros_(self.query.bias)\n",
    "        if self.value.bias is not None:\n",
    "            nn.init.zeros_(self.value.bias)\n",
    "        if self.out.bias is not None:\n",
    "            nn.init.zeros_(self.out.bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D input tensor, got {x.dim()}D\")\n",
    "        if xa is not None and xa.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D cross-attention tensor, got {xa.dim()}D\")\n",
    "            \n",
    "        q = self.query(x)\n",
    "        \n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q=q, k=k, v=v, mask=mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(\n",
    "        self,\n",
    "        q: Tensor,\n",
    "        k: Tensor,\n",
    "        v: Tensor,\n",
    "        mask: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    " \n",
    "        batch, ctx, dims = q.shape\n",
    "        scale = (dims // self.heads) ** -0.25\n",
    "        \n",
    "        q = q.view(batch, ctx, self.heads, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, k.size(1), self.heads, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, v.size(1), self.heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.use_sdpa and torch.cuda.is_available():\n",
    "            with torch.autocast('cuda'):\n",
    "                a = scaled_dot_product_attention(\n",
    "                    query=q,\n",
    "                    key=k,\n",
    "                    value=v,\n",
    "                    is_causal=mask is not None and ctx > 1\n",
    "                )\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:ctx, :ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "        return out, qk\n",
    "    \n",
    "class MultiHeadB(nn.Module):\n",
    "    use_sdpa: bool = True  \n",
    "    def __init__(self, dims: int, heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        if dims % heads != 0:\n",
    "            raise ValueError(f\"dims ({dims}) must be divisible by heads ({heads})\")\n",
    "        if not isinstance(dims, int) or not isinstance(heads, int):\n",
    "            raise TypeError(\"dims and heads must be integers\")\n",
    "            \n",
    "        self.heads = heads\n",
    "        self.dims = dims\n",
    "        self.head_dim = dims // heads\n",
    "        \n",
    "        self.query = Linear(in_features=dims, out_features=dims)\n",
    "        self.key = Linear(in_features=dims, out_features=dims, bias=False)\n",
    "        self.value = Linear(in_features=dims, out_features=dims)\n",
    "        self.out = Linear(in_features=dims, out_features=dims)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.query.weight, std=0.02)\n",
    "        nn.init.normal_(self.key.weight, std=0.02)\n",
    "        nn.init.normal_(self.value.weight, std=0.02)\n",
    "        nn.init.normal_(self.out.weight, std=0.02)\n",
    "        if self.query.bias is not None:\n",
    "            nn.init.zeros_(self.query.bias)\n",
    "        if self.value.bias is not None:\n",
    "            nn.init.zeros_(self.value.bias)\n",
    "        if self.out.bias is not None:\n",
    "            nn.init.zeros_(self.out.bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D input tensor, got {x.dim()}D\")\n",
    "        if xa is not None and xa.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D cross-attention tensor, got {xa.dim()}D\")\n",
    "            \n",
    "        q = self.query(x)\n",
    "        \n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q=q, k=k, v=v, mask=mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(\n",
    "        self,\n",
    "        q: Tensor,\n",
    "        k: Tensor,\n",
    "        v: Tensor,\n",
    "        mask: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    " \n",
    "        batch, ctx, dims = q.shape\n",
    "        scale = (dims // self.heads) ** -0.25\n",
    "        \n",
    "        q = q.view(batch, ctx, self.heads, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, k.size(1), self.heads, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, v.size(1), self.heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.use_sdpa and torch.cuda.is_available():\n",
    "            with torch.autocast('cuda'):\n",
    "                a = scaled_dot_product_attention(\n",
    "                    query=q,\n",
    "                    key=k,\n",
    "                    value=v,\n",
    "                    is_causal=mask is not None and ctx > 1\n",
    "                )\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:ctx, :ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "\n",
    "        return out, qk\n",
    "    \n",
    "class MultiheadC(nn.Module):\n",
    "    use_sdpa: bool = True\n",
    "    def __init__(self, dims: int, heads: int, max_dist: int):\n",
    "        super().__init__()\n",
    "        if dims % heads != 0:\n",
    "            raise ValueError(f\"dims ({dims}) must be divisible by heads ({heads})\")\n",
    "        if dims % 2 != 0:\n",
    "            raise ValueError(f\"dims ({dims}) must be even for rotary embeddings\")\n",
    "        self.heads = heads\n",
    "        self.head_dim = dims // heads\n",
    "        self.dims = dims\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "        scale = 1 / math.sqrt(self.head_dim)\n",
    "        self.query = nn.Linear(dims, dims)\n",
    "        self.key = nn.Linear(dims, dims, bias=False)\n",
    "        self.value = nn.Linear(dims, dims)\n",
    "        self.out = nn.Linear(dims, dims)\n",
    "        \n",
    "        nn.init.normal_(self.query.weight, std=scale)\n",
    "        nn.init.normal_(self.key.weight, std=scale)\n",
    "        nn.init.normal_(self.value.weight, std=scale)\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "        \n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None,\n",
    "                mask: Optional[Tensor] = None, kv_cache: Optional[Dict] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "\n",
    "        q = self.query(x)\n",
    "        \n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q=q, k=k, v=v, mask=mask)\n",
    "        return self.out(wv), qk\n",
    "    \n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "   \n",
    "        batch, ctx, dims = q.shape\n",
    "        scale = (dims // self.heads) ** -0.25\n",
    "        q = q.view(batch, ctx, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, ctx, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, ctx, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.use_sdpa and torch.cuda.is_available():\n",
    "\n",
    "            with torch.autocast('cuda'):\n",
    "                a = scaled_dot_product_attention(\n",
    "                    query=q,\n",
    "                    key=k,\n",
    "                    value=v,\n",
    "                    is_causal=mask is not None and ctx > 1\n",
    "                )\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:ctx, :ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "        return out, qk\n",
    "    \n",
    "class miniAttention(nn.Module):\n",
    "    def __init__(self, dims, max_dist, heads=1, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if dims % heads != 0:\n",
    "            raise ValueError(f\"dims ({dims}) must be divisible by heads ({heads})\")\n",
    "        if dims % 2 != 0:\n",
    "            raise ValueError(f\"dims ({dims}) must be even for rotary embeddings\")\n",
    "        self.heads = heads\n",
    "        self.head_dim = dims // heads\n",
    "        self.dims = dims\n",
    "        self.max_dist = max_dist\n",
    "        self.scale = qk_scale or self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dims, dims * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dims, dims)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        q = q * self.scale\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, param: Dimensions, dims: int, heads: int,\n",
    "                dropout: float, activation: str):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        self.dims = dims\n",
    "\n",
    "        act_fn = nn.GELU() if activation == 'gelu' else \\\n",
    "                nn.ReLU() if activation == 'relu' else \\\n",
    "                nn.Sigmoid() if activation == 'sigmoid' else \\\n",
    "                nn.Tanh() if activation == 'tanh' else \\\n",
    "                nn.LeakyReLU() if activation == 'leaky_relu' else \\\n",
    "                nn.ELU() if activation == 'elu' else \\\n",
    "                nn.ReLU() \n",
    "       \n",
    "        self.attn = MultiheadA(dims=dims, heads=heads)\n",
    "        self.cross = MultiHeadB(dims=dims, heads=heads)\n",
    "        \n",
    "        mlp_dim = dims * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            Linear(in_features=dims, out_features=mlp_dim),\n",
    "            act_fn,\n",
    "            nn.Dropout(p=dropout),\n",
    "            Linear(in_features=mlp_dim, out_features=dims)\n",
    "        )\n",
    "\n",
    "        self.ln_a = LayerNorm(normalized_shape=dims)\n",
    "        self.ln_b = LayerNorm(normalized_shape=dims)\n",
    "        self.ln_c = LayerNorm(normalized_shape=dims)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None):\n",
    "        \n",
    "        y = x\n",
    "        x = x + self.attn(self.ln_a(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        x = x + self.cross(self.ln_b(x), xa, mask=mask, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.ln_c(x))\n",
    "        return x + y\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, param: Dimensions, mels: int, ctx: int, dims: int, heads: int, \n",
    "                checkpoint: bool, dropout: float, activation: str, layerA: int, layerB: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.checkpoint = checkpoint\n",
    "        \n",
    "        act_fn = nn.GELU() if activation == 'gelu' else \\\n",
    "                nn.ReLU() if activation == 'relu' else \\\n",
    "                nn.Sigmoid() if activation == 'sigmoid' else \\\n",
    "                nn.Tanh() if activation == 'tanh' else \\\n",
    "                nn.LeakyReLU() if activation == 'leaky_relu' else \\\n",
    "                nn.ELU() if activation == 'elu' else \\\n",
    "                nn.ReLU() \n",
    "                \n",
    "        self.rotation = rotary(ctx=ctx, dims=dims, heads=heads, base=10000)\n",
    "        self.position = sinusoids(length=ctx, channels=dims)\n",
    "        self.register_buffer(name=\"positions\", tensor=self.position, persistent=False)\n",
    "  \n",
    "        self.convx = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=mels, out_channels=dims, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(num_features=dims), \n",
    "            act_fn,  \n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv1d(in_channels=dims, out_channels=dims, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(num_features=dims), act_fn, nn.Dropout(p=dropout))\n",
    "        \n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(tensor=m.weight)\n",
    "        self.convx.apply(init_weights)\n",
    "                \n",
    "        self.blockA = nn.ModuleList(modules=[Residual(param=param, dims=dims, heads=heads, \n",
    "                dropout=dropout, activation=activation) \n",
    "                for _ in range(layerB)]) if layerB > 0 else None\n",
    "        \n",
    "        self.blockB = nn.ModuleList(modules=[Residual(param=param, dims=dims, heads=heads, \n",
    "                dropout=dropout, activation=activation) \n",
    "                for _ in range(layerA)]) if layerA > 0 else None\n",
    "               \n",
    "        self.ln_post = LayerNorm(normalized_shape=dims)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        x = checkpoint(self._conv_forward, x, use_reentrant=True) if self.checkpoint else self._conv_forward(x)\n",
    "        for block in chain(self.blockA or [], \n",
    "                           self.blockB or []):\n",
    "            x = checkpoint(block, x, use_reentrant=True) if self.checkpoint else block(x)\n",
    "        return self.ln_post(x)\n",
    "\n",
    "    def _conv_forward(self, x) -> Tensor:\n",
    "        x = F.gelu(self.convx(x))\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x = (x + self.positions).to(x.dtype)  # type: ignore\n",
    "        x = self.rotation(x)\n",
    "        return x\n",
    "   \n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, param: Dimensions, vocab: int, ctx: int, dims: int, heads: int, \n",
    "                checkpoint: bool, dropout: float, activation: str, layerA: int, layerB: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.checkpoint = checkpoint\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab, embedding_dim=dims)\n",
    "        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(torch.empty(ctx, dims))\n",
    "        nn.init.normal_(self.positional_embedding, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(ctx=ctx, dims=dims)\n",
    "        self.ln = LayerNorm(normalized_shape=dims)\n",
    "        \n",
    "        self.blockA = nn.ModuleList([Residual(param=param, dims=dims, heads=heads, \n",
    "                dropout=dropout, activation=activation) \n",
    "                for _ in range(layerB)]) if layerB > 0 else None\n",
    "        \n",
    "        self.blockB = nn.ModuleList([Residual(param=param, dims=dims, heads=heads, \n",
    "                dropout=dropout, activation=activation) \n",
    "                for _ in range(layerA)]) if layerA > 0 else None\n",
    "\n",
    "        mask = torch.empty(ctx, ctx).fill_(-np.inf).triu_(diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "        self.mask = mask\n",
    "        \n",
    "    def forward(self, x, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "          \n",
    "        x = checkpoint(function=self._embedding_forward, x=x, xa=xa, kv_cache=kv_cache) if self.checkpoint else self._embedding_forward(x=x, xa=xa, kv_cache=kv_cache)\n",
    "        \n",
    "        for block in chain(self.blockA or [], self.blockB or []):        \n",
    "            x = checkpoint(function=block, x=x, xa=xa, mask=self.mask, kv_cache=kv_cache) if self.checkpoint else block(x=x, xa=xa, mask=self.mask, kv_cache=kv_cache)\n",
    "        x = self.ln(x)\n",
    "               \n",
    "        x = (x @ torch.transpose(self.token_embedding.weight.to(dtype=x.dtype), dim0=0, dim1=1)).float()\n",
    "        return x\n",
    "    \n",
    "    def _embedding_forward(self, x, xa, kv_cache):       \n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        x = (self.token_embedding(x) + self.positional_embedding[offset : offset + x.shape[-1]])\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.to(dtype=xa.dtype)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Echo(nn.Module):\n",
    "    def __init__(self, param: Dimensions):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        self.device_param = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.encoder = AudioEncoder(\n",
    "            param=self.param,\n",
    "            mels=self.param.mels,\n",
    "            ctx=self.param.audio_ctx,\n",
    "            dims=self.param.audio_state,\n",
    "            heads=self.param.audio_head,\n",
    "            layerA=self.param.audio_layerA,\n",
    "            layerB=self.param.audio_layerB,\n",
    "            checkpoint=self.param.checkpoint,\n",
    "            dropout=self.param.dropout,\n",
    "            activation=self.param.activation,\n",
    "\n",
    "        ).to(device=self.device_param)\n",
    "        \n",
    "        self.decoder = TextDecoder(\n",
    "            param=self.param,\n",
    "            vocab=self.param.vocab,\n",
    "            ctx=self.param.text_ctx,\n",
    "            dims=self.param.text_state,\n",
    "            heads=self.param.text_head,\n",
    "            layerA=self.param.text_layerA,\n",
    "            layerB=self.param.text_layerB,\n",
    "            checkpoint=self.param.checkpoint,\n",
    "            dropout=self.param.dropout,\n",
    "            activation=self.param.activation,\n",
    "         \n",
    "        ).to(device=self.device_param)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.param.text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiheadA):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "            \n",
    "    def save_pretrained(self, save_directory: str, save_config: bool = True, safe_serialization: bool = False):\n",
    "        if os.path.isfile(save_directory):\n",
    "            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        weights_file = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        if safe_serialization:\n",
    "            try:\n",
    "                from safetensors.torch import save_file as safe_save_file\n",
    "                state_dict = self.state_dict()\n",
    "                safe_save_file(state_dict, weights_file.replace(\".bin\", \".safetensors\"))\n",
    "            except ImportError:\n",
    "                warnings.warn(\"safetensors not found. Falling back to torch.save\")\n",
    "                torch.save(self.state_dict(), weights_file)\n",
    "        else:\n",
    "            torch.save(self.state_dict(), weights_file)\n",
    "\n",
    "        if save_config:\n",
    "            config_dict = {\n",
    "                \"model_config\": self.param.to_dict(),\n",
    "                \"architectures\": [self.__class__.__name__],\n",
    "                \"model_type\": \"whisper\",\n",
    "                \"vocab_size\": self.param.vocab,\n",
    "                \"decoder_dims\": self.param.text_state,\n",
    "                \"encoder_dims\": self.param.audio_state,\n",
    "                \"decoder_attention_heads\": self.param.text_head,\n",
    "                \"encoder_attention_heads\": self.param.audio_head,   \n",
    "                \"encoder_layers\": self.param.audio_layerA,\n",
    "                \"decoder_layers\": self.param.text_layerA,\n",
    "                \"dropout\": self.param.dropout}\n",
    "\n",
    "            config_file = os.path.join(save_directory, \"config.json\")\n",
    "            with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(config_dict, f, indent=2, sort_keys=True)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        pretrained_model_name_or_path: str,\n",
    "        device_map: Optional[str] = None,\n",
    "        torch_dtype: Optional[torch.dtype] = None,\n",
    "        force_cpu: bool = False\n",
    "    ) -> \"Echo\":\n",
    "\n",
    "        if os.path.isfile(pretrained_model_name_or_path):\n",
    "            raise ValueError(f\"Provided path ({pretrained_model_name_or_path}) should be a directory, not a file\")\n",
    "\n",
    "        config_file = os.path.join(pretrained_model_name_or_path, \"config.json\")\n",
    "        if not os.path.exists(config_file):\n",
    "            raise ValueError(f\"Config file not found in {pretrained_model_name_or_path}\")\n",
    "\n",
    "        with open(file=config_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            config_dict = json.load(fp=f)\n",
    "\n",
    "        model_config = config_dict.get(\"model_config\", config_dict)\n",
    "        model = cls(param=Dimensions.from_dict(config=model_config))\n",
    "\n",
    "        if force_cpu:\n",
    "            device = torch.device(\"cpu\")\n",
    "        elif device_map is not None:\n",
    "            device = torch.device(device_map)\n",
    "        else:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if torch_dtype is not None:\n",
    "            model = model.to(torch_dtype)\n",
    "\n",
    "        weights_path = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n",
    "        safetensors_path = os.path.join(pretrained_model_name_or_path, \"pytorch_model.safetensors\")\n",
    "        if os.path.exists(safetensors_path):\n",
    "            try:\n",
    "                from safetensors.torch import load_file as safe_load_file\n",
    "                state_dict = safe_load_file(safetensors_path)\n",
    "            except ImportError:\n",
    "                warnings.warn(\"safetensors not found. Falling back to torch.load\")\n",
    "                state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
    "        elif os.path.exists(weights_path):\n",
    "            state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
    "        else:\n",
    "            raise ValueError(f\"No weights found in {pretrained_model_name_or_path}\")\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(device)\n",
    "        return model\n",
    "        \n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids, pad_token_id=50257, decoder_start_token_id=50258):\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone() \n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features, labels=None, dec_input_ids=None) -> dict[str, Any | None]:\n",
    "        if labels is not None:\n",
    "            if dec_input_ids is None:\n",
    "                dec_input_ids = self.shift_tokens_right(\n",
    "                    input_ids=labels, pad_token_id=50257, decoder_start_token_id=50258\n",
    "                )\n",
    "\n",
    "        encoded_features = self.encoder(input_features).to(self.device)  \n",
    "        logits = self.decoder(dec_input_ids, encoded_features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            labels = labels.to(logits.device).long()\n",
    "            loss = loss_fct(logits.view(-1, self.param.vocab), labels.view(-1))    \n",
    " \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, (Linear, Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, AudioEncoder):\n",
    "            module.convx.apply(fn=self._init_weights)\n",
    "        elif isinstance(module, TextDecoder):\n",
    "            nn.init.normal_(tensor=module.positional_embedding, mean=0.0, std=std)\n",
    "            nn.init.normal_(tensor=module.token_embedding.weight, mean=0.0, std=std)\n",
    "        elif isinstance(module, Residual):\n",
    "            for layer in module.mlp:\n",
    "                if isinstance(layer, Linear):\n",
    "                    nn.init.normal_(tensor=layer.weight, std=std)\n",
    "                    nn.init.zeros_(tensor=layer.bias)\n",
    "                nn.init.normal_(tensor=LayerNorm(normalized_shape=module.dims).weight, mean=0.0, std=std)\n",
    "                nn.init.normal_(tensor=LayerNorm(normalized_shape=module.dims).bias, mean=0.0, std=std) \n",
    "                module.attn.init_weights()\n",
    "                module.cross.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.apply(fn=self._init_weights)\n",
    "\n",
    "from datetime import datetime\n",
    "log_dir = os.path.join('./output/echo', datetime.now().strftime(format='%m-%d_%H'))\n",
    "os.makedirs(name=log_dir, exist_ok=True)\n",
    "\n",
    "param = Dimensions(\n",
    "        mels = 80,\n",
    "        audio_ctx = 1500,\n",
    "        audio_head = 4,\n",
    "        audio_layerA = 4,\n",
    "        audio_layerB = 0,\n",
    "        audio_state = 512,\n",
    "        vocab = 51865,\n",
    "        text_ctx = 448,\n",
    "        text_head = 4, \n",
    "        text_layerA = 4,\n",
    "        text_layerB = 0,\n",
    "        text_state = 512,\n",
    "        checkpoint = False,\n",
    "        dropout = 0.01,\n",
    "        activation = 'gelu',\n",
    "        )\n",
    "\n",
    "model = Echo(param=param).to(device=device)\n",
    "model.init_weights()\n",
    "\n",
    "\n",
    "token=\"\"\n",
    "\n",
    "extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"openai/whisper-small\", token=token)\n",
    "\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"openai/whisper-small\", \n",
    "    language=\"en\", task=\"transcribe\", token=token)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"openai/whisper-small\", token=token)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    extractor: Any\n",
    "    tokenizer: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], Tensor]]]) -> Dict[str, Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "    \n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor, extractor=extractor,\n",
    "    tokenizer=tokenizer, decoder_start_token_id=50258)\n",
    "\n",
    "datasets = IterableDatasetDict()\n",
    "\n",
    "datasets[\"train\"] = load_dataset(\n",
    "    path=\"mozilla-foundation/common_voice_17_0\",\n",
    "    name=\"en\", split=\"train\", streaming=True, \n",
    "    token=token, trust_remote_code=True)#.take(10000)\n",
    "\n",
    "datasets[\"test\"] = load_dataset(\n",
    "    path=\"mozilla-foundation/common_voice_17_0\", \n",
    "    name=\"en\", split=\"test\", streaming=True, \n",
    "    token=token, trust_remote_code=True).take(500) # type: ignore\n",
    "\n",
    "dataset = datasets.cast_column(column=\"audio\", feature=Audio(sampling_rate=16000))\n",
    "\n",
    "dataset = dataset.map(function=prepare_dataset, \n",
    "    remove_columns=list(next(iter(dataset.values()))\n",
    "                        .features)).with_format(type=\"torch\")\n",
    "\n",
    "metric = evaluate.load(path=\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    pred_logits = eval_pred.predictions\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    if isinstance(pred_logits, tuple):\n",
    "        pred_ids = pred_logits[0]\n",
    "    else:\n",
    "        pred_ids = pred_logits\n",
    "    if pred_ids.ndim == 3:\n",
    "        pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str) # type: ignore\n",
    "    pred_flat = pred_ids.flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    mask = labels_flat != tokenizer.pad_token_id\n",
    "  \n",
    "    if len(pred_str) > 0:\n",
    "        sample_idx = random.randint(0, len(pred_str) - 1)\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"Prediction: {pred_str[sample_idx]}\")\n",
    "        print(f\"Label: {label_str[sample_idx]}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "    acc = accuracy_score(y_true=labels_flat[mask], y_pred=pred_flat[mask])\n",
    "    pre = precision_score(y_true=labels_flat[mask], y_pred=pred_flat[mask], \n",
    "    average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true=labels_flat[mask], y_pred=pred_flat[mask], \n",
    "    average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true=labels_flat[mask], y_pred=pred_flat[mask], \n",
    "    average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": pre,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1}\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=log_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    max_steps=100000,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    warmup_steps=300,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    logging_dir=log_dir + \"/logs_hf\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    eval_on_start=False,\n",
    "    optim=\"adafactor\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"test\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=extractor,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
